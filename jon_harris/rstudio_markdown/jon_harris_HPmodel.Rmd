---
title: "Kaggle"
author: "Jonathan Harris"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
```

## Load Packages
```{r}

library(dplyr)    #data manipulation
library(ggplot2)  #ploting
library(xgboost)  #xgboost for gradient tree model
library(caret)    #for ML

```

#Import Data
```{r}

setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) #set directory
load('train_all.RData') #load original and featured-engineered datasets

hp_feat = hp_feat %>% select(-SalePrice) #remove ID and sales price
hp_orig = hp_orig %>% rename(SalePrice = SalePrice_y, SalePrice_log = SalePrice_log_y) %>%
  select(-SalePrice) #correct name from merging datasets

train_idx = sample(1:nrow(hp_feat), .7*nrow(hp_feat))

```

#XGBoost Model 1: Feature-Engineered Dataset
```{r}

#Prepare predictor and response matrices
predictors_train = data.matrix(hp_feat %>% select(-SalePrice_log, -Id)) #must be matrix
response_train = data.matrix(hp_feat %>% select(SalePrice_log, -Id))    #must be matrix

#2nd model: original data, no hypertuning
xgb.feat <- xgb.cv(
  data = predictors_train,      #Predictor variables
  label = response_train,       #Response variable
  nrounds = 1000,               #Num of trees
  nfold = 5,                    #Number of folds (~5-10)
  objective = "reg:linear",     #Linear regression
  verbose = 0,                  #No output
  early_stopping_rounds = 10    #haults CV if MSE doesn't improve
)

#Calculate RMSE of train and test 
  #RMSE_train = ~0.0266318
  #RMSE_test = ~0.1416268
xgb.feat$evaluation_log %>%
  summarise(
    rmse_train = min(train_rmse_mean),
    rmse_test = min(test_rmse_mean),
  )

#Plot RMSE vs Number of Trees
ggplot(xgb.feat$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean, color = 'Train')) +
  geom_line(aes(iter, test_rmse_mean, color = 'Test')) +
  labs(x='Number of Trees', y='RMSE', title='Feature-Engineer Dataset: Train RMSE vs. Number of Trees')

#Create model XGBoost model
xgb.feat_test <- xgboost(
  data = predictors_train,      #Predictor variables
  label = response_train,       #Response variable
  nrounds = 1000,               #Num of trees
  nfold = 5,                    #Number of folds (~5-10)
  objective = "reg:linear",     #Linear regression
  verbose = 0,                  #No output
)

#Plot variable importance of 'test' subset of train data
importance_matrix = xgb.importance(model = xgb.feat_test) #create matrix
xgb.plot.importance(importance_matrix, top_n = 20, measure = "Gain") 

#Predict SalePrice_log of 'test' subset of train data 
test_df = data.matrix(hp_feat[-train_idx,] %>% select(-Id, -SalePrice_log)) #isolate test data
xgb_pred = predict(xgb.feat_test, test_df) #predict saleprice_log

#Calculate MSE of 'test' subset of train data
  #MSE = 2.128978e-06
MSE.feat = mean((xgb_pred - hp_feat$SalePrice_log[-train_idx])^2)
MSE.feat

#How much is the prediction price off from the actual 'test' price?
pred_dol = exp(xgb_pred) #predicted house price [$]
act_dol = exp(hp_feat$SalePrice_log[-train_idx]) #actual house price (test subset of training dataset) [$]
mean(abs(pred_dol-act_dol))

```

#XGBoost Model 2: Original Dataset
```{r}

#Prepare predictor and response matrices
predictors_train = data.matrix(hp_orig %>% select(-SalePrice_log, -Id)) #must be matrix
response_train = data.matrix(hp_orig %>% select(SalePrice_log, -Id))    #must be matrix

#2nd model: original data, no hypertuning
xgb.orig <- xgb.cv(
  data = predictors_train,      #Predictor variables
  label = response_train,       #Response variable
  nrounds = 1000,               #Num of trees
  nfold = 5,                    #Number of folds (~5-10)
  objective = "reg:linear",     #Linear regression
  verbose = 0,                  #No output
  early_stopping_rounds = 10    #haults CV if MSE doesn't improve
)

#Calculate RMSE of train and test 
  #RMSE_train = ~0.0082528		
  #RMSE_test = ~0.1338334
xgb.orig$evaluation_log %>%
  summarise(
    rmse_train = min(train_rmse_mean),
    rmse_test = min(test_rmse_mean),
  )

#Plot RMSE vs Number of Trees
ggplot(xgb.orig$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean, color = 'Train')) +
  geom_line(aes(iter, test_rmse_mean, color = 'Test')) +
  labs(x='Number of Trees', y='RMSE', title='Original Dataset: Train RMSE vs. Number of Trees')

#Create model XGBoost model
xgb.orig_test <- xgboost(
  data = predictors_train,      #Predictor variables
  label = response_train,       #Response variable
  nrounds = 1000,               #Num of trees
  nfold = 5,                    #Number of folds (~5-10)
  objective = "reg:linear",     #Linear regression
  verbose = 0,                  #No output
)

#Plot variable importance of 'test' subset of train data
importance_matrix = xgb.importance(model = xgb.orig_test) #create matrix
xgb.plot.importance(importance_matrix, top_n = 20, measure = "Gain") 

#Predict SalePrice_log of 'test' subset of train data 
test_df = data.matrix(hp_orig[-train_idx,] %>% select(-Id, -SalePrice_log)) #isolate test data
xgb_pred = predict(xgb.orig_test, test_df) #predict saleprice_log

#Calculate MSE of 'test' subset of train data
  #MSE.orig = 1.555677e-06
MSE.orig = mean((xgb_pred - hp_orig$SalePrice_log[-train_idx])^2)
MSE.orig

#How much is the prediction price off from the actual 'test' price?
pred_dol = exp(xgb_pred) #predicted house price [$]
act_dol = exp(hp_orig$SalePrice_log[-train_idx]) #actual house price (test subset of training dataset) [$]
mean(abs(pred_dol-act_dol))

```


#XGBoost Model3: Combine Original and Featured Datasets
```{r}

#Previously, model2 (orig df) outperformed model1 (df)
#Want to combine to maximize best columns
uniq = c(setdiff(names(hp_feat),names(hp_orig)), 'Id')
combined = merge(hp_orig, hp_feat[uniq], by='Id')
combined_train = combined[train_idx,]

#Prep Feature and Response Data
predictors_train = data.matrix(combined_train %>% select(-SalePrice_log, -Id)) #must be matrix
response_train = data.matrix(combined_train %>% select(SalePrice_log, -Id))    #must be matrix

#3rd Model: Combined Variables, No Hyperparameter Tuning
xgb.comb <- xgb.cv(
  data = predictors_train,
  label = response_train,
  nrounds = 1000,               #Num of trees
  nfold = 5,                    #Number of folds (~5-10)
  objective = "reg:linear",     #Linear regression
  verbose = 0,                  #No output
  early_stopping_rounds = 10    #haults CV if MSE doesn't improve
)

#Calculate RMSE of train and test 
  #RMSE_train = ~0.0069476		
  #RMSE_test = ~0.142179
xgb.comb$evaluation_log %>%
  summarise(
    rmse_train   = min(train_rmse_mean),
    rmse_test   = min(test_rmse_mean),
  )

#Plot RMSE vs Number of Trees
ggplot(xgb.orig$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean, color = 'Train')) +
  geom_line(aes(iter, test_rmse_mean, color = 'Test')) +
  labs(x='Number of Trees', y='RMSE', title='Combined Dataset: Train RMSE vs. Number of Trees')

#Create model XGBoost model
xgb.comb_test <- xgboost(
  data = predictors_train,
  label = response_train,
  nrounds = 1000,               #Num of trees
  nfold = 5,                    #Number of folds (~5-10)
  objective = "reg:linear",     #Linear regression
  verbose = 0,                  #No output
)

#Plot variable importance of 'test' subset of train data
importance_matrix = xgb.importance(model = xgb.comb_test)
xgb.plot.importance(importance_matrix, top_n = 20, measure = "Gain")

#Predict SalePrice_log of 'test' subset of train data 
test_df = data.matrix(combined[-train_idx,] %>% select(-Id, -SalePrice_log))
xgb_pred = predict(xgb.comb_test, test_df)

#Calculate MSE of 'test' subset of train data
  #MSE.comb = 0.01627476
MSE.comb = mean((xgb_pred - combined$SalePrice_log[-train_idx])^2)
MSE.comb

#How much is the prediction price off from the actual 'test' price?
pred_dol = exp(xgb_pred) #predicted house price [$]
act_dol = exp(combined$SalePrice_log[-train_idx]) #actual house price (test subset of training dataset) [$]
mean(abs(pred_dol-act_dol))

```