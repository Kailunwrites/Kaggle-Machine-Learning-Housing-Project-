---
title: "Kaggle"
author: "Jonathan Harris"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
```

## Load Packages
```{r}

library(dplyr)    #data manipulation
library(ggplot2)  #ploting
library(xgboost)  #xgboost for gradient tree model
library(moments)  #calculating skewness
library(caret)    #ML

```

#Import Data
```{r}

setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) #set directory
load('train_all_v2.RData') #load original and featured-engineered datasets

```


#XGBoost Model 1: Original Dataset
```{r}

#Prepare predictor and response matrices
predictors_train = data.matrix(hp_orig %>% select(-SalePrice_log, -Id)) #must be matrix
response_train = data.matrix(hp_orig %>% select(SalePrice_log, -Id))    #must be matrix

#1st model: original data, no hypertuning
xgb.orig = xgb.cv(
  data = predictors_train,     #Predictor variables
  label = response_train,      #Response variable
  nrounds = 1000,              #Num of trees
  nfold = 5,                   #Number of folds (~5-10)
  objective = "reg:linear",    #Linear regression
  verbose = 0,                 #No output
  early_stopping_rounds = 20,  #haults CV if MSE doesn't improve after 10 trees
)

#Calculate RMSE of train and test
#RMSE_train = ~ 0.0011762		
#RMSE_test = ~ 0.1339616
xgb.orig$evaluation_log %>%
  summarise(rmse_train = min(train_rmse_mean),
            rmse_test = min(test_rmse_mean),)

#Plot RMSE vs Number of Trees
ggplot(xgb.orig$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean, color = 'Train')) +
  geom_line(aes(iter, test_rmse_mean, color = 'Test')) +
  labs(x = 'Number of Trees', y = 'RMSE', title = 'Original Dataset: Train RMSE vs. Number of Trees') + coord_cartesian(ylim = c(0, .2))

# #Create model XGBoost model
# xgb.orig_test <- xgboost(
#   data = predictors_train,
#   #Predictor variables
#   label = response_train,
#   #Response variable
#   nrounds = 1000,
#   #Num of trees
#   nfold = 5,
#   #Number of folds (~5-10)
#   objective = "reg:linear",
#   #Linear regression
#   verbose = 0,
#   #No output
# )
# 
# #Plot variable importance of 'test' subset of train data
# importance_mat.orig = xgb.importance(model = xgb.orig_test) #create matrix
# xgb.plot.importance(importance_mat.orig, top_n = 20, measure = "Gain")
# 
# #Predict SalePrice_log of 'test' subset of train data
# test_df = data.matrix(hp_orig[-train_idx, ] %>% select(-Id, -SalePrice_log)) #isolate test data
# xgb_pred.orig = predict(xgb.orig_test, test_df) #predict saleprice_log
# 
# #Calculate MSE of 'test' subset of train data
# #MSE.orig = 1.555677e-06
# MSE.orig = mean((xgb_pred.orig - hp_orig$SalePrice_log[-train_idx]) ^ 2)
# MSE.orig
# 
# #How much is the prediction price off from the actual 'test' price?
# pred_dol = exp(xgb_pred.orig) #predicted house price [$]
# act_dol = exp(hp_orig$SalePrice_log[-train_idx]) #actual house price (test subset of training dataset) [$]
# mean(abs(pred_dol - act_dol))

```

#XGBoost Model 2: Original Dataset + Feature Engineered Continuous Variables
```{r}

set.seed(249) #determined with forloop

#Add non-categorical variables from featured dataset to original dataset
df1 = hp_orig %>% select(-GarageCars)
df2 = hp_feat %>% select(TotalSF, Age, AgeRemod, TotPorchSF, TotCarGarage, TotBaths)
orig_cont = cbind.data.frame(df1, df2)

#Prepare predictor and response matrices
predictors_train = data.matrix(orig_cont %>% select(-SalePrice_log, -Id)) #must be matrix
response_train = data.matrix(orig_cont %>% select(SalePrice_log, -Id))    #must be matrix

#1st model: original data, no hypertuning
xgb.orig_contfeat = xgb.cv(
  data = predictors_train,     #Predictor variables
  label = response_train,      #Response variable
  nrounds = 1000,              #Num of trees
  nfold = 5,                   #Number of folds (~5-10)
  objective = "reg:linear",    #Linear regression
  verbose = 0,                 #No output
  early_stopping_rounds = 20,  #haults CV if MSE doesn't improve after 10 trees
)

#Calculate RMSE of train and test
  #RMSE_train = ~ 0.0033382	
  #RMSE_test = ~ 0.1293408
xgb.orig_contfeat$evaluation_log %>%
  summarise(rmse_train = min(train_rmse_mean),
            rmse_test = min(test_rmse_mean),
  )

#Plot RMSE vs Number of Trees
ggplot(xgb.orig$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean, color = 'Train')) +
  geom_line(aes(iter, test_rmse_mean, color = 'Test')) +
  labs(x = 'Number of Trees', y = 'RMSE', title = 'Original Dataset: Train RMSE vs. Number of Trees') + coord_cartesian(ylim = c(0, .2))

```

#XGBoost Model 3: Tune Hyperparameters
```{r}

set.seed(249) #determined with forloop

#Create hypergrid of hyperparameters to tune
hyper_grid <- expand.grid(
  eta = c(0.05),                    #learning rate
  gamma = c(0.05),                  #min loss reduction
  max_depth = c(4),             #min weight sum required in a child
  min_child_weight = c(1.20, 1.25, 1.3),  #max depth of trees
  subsample = c(0.3),     #ratio of training predictors 
  alpha = c(0.1, .15, 0.2),       #L1 lasso penalization
  lambda = c(0.6, 0.65, 0.7)        #L2 lasso penalization
)

nrow(hyper_grid)

#Perform Grid Search
for(i in 1:nrow(hyper_grid)) {
  
  #Create parameter list based on iterative hyper grid inputs
  params = list(
    eta = hyper_grid$eta[i],
    gamma = hyper_grid$eta[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    alpha = hyper_grid$alpha[i],
    lambda = hyper_grid$lambda[i]
  )
  
  # ========================================================
  # min_RMSE = 0.1137642
  # ========================================================
  # params = list(
  #   eta = 0.05,
  #   gamma = 0.05
  #   max_depth = 4,
  #   min_child_weight = 1.2,
  #   subsample = 0.3,
  #   alpha = 0.2,
  #   lambda = 0.7,
  #   nrounds = 282,
  # )
  # ========================================================
  
  #Cross-Valication
  xgb.orig_tran_tune = xgb.cv(
    params = params,
    data = predictors_train,     #Predictor variables
    label = response_train,      #Response variable
    nrounds = 500,              #Num of trees
    nfold = 5,                   #Number of folds (~5-10)
    objective = "reg:linear",    #Linear regression
    verbose = 0,                 #No output
    early_stopping_rounds = 10,  #haults CV if MSE doesn't improve after 10 trees
  )
  
  # add min training error and trees to grid
  hyper_grid$min_trees[i] <- which.min(xgb.orig_tran_tune$evaluation_log$test_rmse_mean)
  hyper_grid$min_RMSE[i] <- min(xgb.orig_tran_tune$evaluation_log$test_rmse_mean)
}

ans = hyper_grid %>% arrange(min_RMSE) %>% head(10)
ans[1,]

```









#XGBoost Model 3: Transform Continuous Variables
```{r}

#Prepare predictor and response matrices
predictors_train = data.matrix(orig_cont %>% select(-SalePrice_log, -Id)) #must be matrix
response_train = data.matrix(orig_cont %>% select(SalePrice_log, -Id))    #must be matrix

#Create hypergrid of hyperparameters to tune
hyper_grid <- expand.grid(
  eta = c(0.005, 0.01),           #learning rate
  max_depth = c(3,4,5),           #min weight sum required in a child
  min_child_weight = c(3,4),    #max depth of trees
  subsample = c(0.20, .3),           #ratio of training predictors 
  colsample_bytree = c(1),     #subsample ratio per tree
  alpha = c(0.005, 0.01)                            #L1 lasso penalization
)

nrow(hyper_grid)

#Perform Grid Search
for(i in 1:nrow(hyper_grid)) {
  
  #Create parameter list based on iterative hyper grid inputs
  params = list(
    eta = hyper_grid$eta[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i],
    alpha = hyper_grid$alpha[i]
  )
  
  # ========================================================
  # min_RMSE = 0.1124
  # ========================================================
  # params = list(
  #   eta = 0.01,
  #   max_depth = 4,
  #   min_child_weight = 3,
  #   subsample = 0.25,
  #   colsample_bytree = 1,
  #   alpha = 0.01,
  #   nrounds = 1207,
  # )
  # ========================================================
  #Cross-Valication
  xgb.orig_tran_tune = xgb.cv(
    params = params,
    data = predictors_train,     #Predictor variables
    label = response_train,      #Response variable
    nrounds = 2000,              #Num of trees
    nfold = 5,                   #Number of folds (~5-10)
    objective = "reg:linear",    #Linear regression
    verbose = 0,                 #No output
    early_stopping_rounds = 10,  #haults CV if MSE doesn't improve after 10 trees
  )
  
  # add min training error and trees to grid
  hyper_grid$min_trees[i] <- which.min(xgb.orig_tran_tune$evaluation_log$test_rmse_mean)
  hyper_grid$min_RMSE[i] <- min(xgb.orig_tran_tune$evaluation_log$test_rmse_mean)
}

ans = hyper_grid %>% arrange(min_RMSE) %>% head(10)
ans[1,]


```




#XGBoost Model 2: Feature-Engineered Dataset
```{r}

# #Transform continuous data if skewed
# cont_var = c(
#   'LotFrontage',
#   'LotArea',
#   'MasVnrArea',
#   'BsmtFinSF1',
#   'BsmtFinSF2',
#   'BsmtUnfSF',
#   'TotalBsmtSF',
#   'X1stFlrSF',
#   'X2ndFlrSF',
#   'LowQualFinSF',
#   'GrLivArea',
#   'GarageYrBlt',
#   'GarageArea',
#   'WoodDeckSF',
#   'OpenPorchSF',
#   'EnclosedPorch',
#   'X3SsnPorch',
#   'ScreenPorch',
#   'PoolArea'
# )
# 
# #Transform if Skewed
# for (var in cont_var) {
#   skew = abs(skewness(hp_feat[var]))
#   
#   if (skew > 0.8) {
#     hp_feat[var] = log(hp_feat[var] + 1)
#   }
# }

#Prepare predictor and response matrices
predictors_train = data.matrix(hp_feat %>% select(-SalePrice_log, -Id)) #must be matrix
response_train = data.matrix(hp_feat %>% select(SalePrice_log, -Id))    #must be matrix

#Model: feature-engineered data, no hypertuning
xgb.feat = xgb.cv(
  data = predictors_train,
  #Predictor variables
  label = response_train,
  #Response variable
  nrounds = 1000,
  #Num of trees
  nfold = 5,
  #Number of folds (~5-10)
  objective = "reg:linear",
  #Linear regression
  verbose = 0,
  #No output
  early_stopping_rounds = 10,
  #haults CV if MSE doesn't improve
  save_name = 'xgb.feat.model' #save model
)

#Calculate RMSE of train and test
#RMSE_train = ~0.037697
#RMSE_test = ~0.1365288
xgb.feat$evaluation_log %>%
  summarise(rmse_train = min(train_rmse_mean),
            rmse_test = min(test_rmse_mean))

#Plot RMSE vs Number of Trees
ggplot(xgb.feat$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean, color = 'Train')) +
  geom_line(aes(iter, test_rmse_mean, color = 'Test')) +
  labs(x = 'Number of Trees', y = 'RMSE', title = 'Feature-Engineer Dataset: Train RMSE vs. Number of Trees') +
  coord_cartesian(ylim = c(0, .2))

# #Create model XGBoost model
# xgb.feat_test <- xgboost(
#   data = predictors_train,
#   #Predictor variables
#   label = response_train,
#   #Response variable
#   nrounds = 1000,
#   #Num of trees
#   nfold = 5,
#   #Number of folds (~5-10)
#   objective = "reg:linear",
#   #Linear regression
#   verbose = 0,
#   #No output
# )
#
# #Plot variable importance of 'test' subset of train data
# importance_mat.feat = xgb.importance(model = xgb.feat_test) #create matrix
# xgb.plot.importance(importance_mat.feat, top_n = 20, measure = "Gain")
#
# #Predict SalePrice_log of 'test' subset of train data
# test_df = data.matrix(hp_feat[-train_idx, ] %>% select(-Id, -SalePrice_log)) #isolate test data
# xgb_pred.feat = predict(xgb.feat_test, test_df) #predict saleprice_log
#
# #Calculate MSE of 'test' subset of train data
# #MSE = 2.128978e-06
# MSE.feat = mean((xgb_pred.feat - hp_feat$SalePrice_log[-train_idx]) ^ 2)
# MSE.feat
#
# #How much is the prediction price off from the actual 'test' price?
# pred_dol = exp(xgb_pred.feat) #predicted house price [$]
# act_dol = exp(hp_feat$SalePrice_log[-train_idx]) #actual house price (test subset of training dataset) [$]
# mean(abs(pred_dol - act_dol))

```

#Additional Feature Engineering
```{r}

# - eta (default = 0.03) 
# 	- Learning rate
# 	- Typically 0.01-0.02
# 
# - min_child_weight
# 	- min sum of weights of all obs required in a child
# 
# - max_depth (default = 6)
# 	- Max depth of trees
# 	- Typically 3-10
# 
# - max_leaf_nodes
# 	- max number of terminal nodes/leaves in a tree
# 
# - subsample
# 	- Denotes fraction of obs to be randomly sampled for each tree
# 	- Typically 0.5-1
# 
# - colsample_bytree
# 	- Denotes subsample ratio of col for each split
# 
# - alpha
# 	- l1 regularization (lasso) term on weight
# 
# - scale_pos_weight (default = 1)
# 	- >0 in case of high class imbalance



```