---
title: "Kaggle"
author: "Jonathan Harris"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
```

## Load Packages
```{r}

library(tidyverse)
library(dplyr)
library(ggplot2)

```

#Import Data
```{r}
df = read_csv("train_all.csv")
head(df)
View(df)

```

3. Fit a Model: Fit a multiple linear regression predicting the price of a meal based on the customer views and location of the restaurant. For this model:
a. Write out the regression equation.
b. Interpret the meaning each of the 5 coefficients in context of the problem.
c. Are the coefficients significant? How can you tell?
d. Is the overall regression significant? How can you tell?
e. Find and interpret the RSE.
f. Find and interpret the adjusted coefficient of determination.
```{r}


numericVars <- which(sapply(all, is.numeric)) #index vector numeric variables
numericVarNames <- names(numericVars) #saving names vector for use later on
cat('There are', length(numericVars), 'numeric variables')

all_numVar <- all[, numericVars]
cor_numVar <- cor(all_numVar, use="pairwise.complete.obs") #correlations of all numeric variables

#sort on decreasing correlations with SalePrice
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))
 #select only high corelations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]

corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt")

```

4. Diagnostics: Investigate the assumptions of the model using the plot() function. Are there any violations?

```{r}

plot(model.saturated)

#Resid vs Fittede: Uniform variance of residual
#Normal QQ: Residuals fairly linear
#Scale-Location: Homoscedasticity
#Resid vs Leverage: No outliers due to cook, but two points influence results

```

5. Outliers: Investigate the influence plot for the model. Are there any restaurants about which we should be concerned?
```{r}

library(car) #Companion to applied regression.
influencePlot(model.saturated)

#Points 117 and 168 

```

6. Multicollinearity: Investigate the coefficient variance inflation factors; use these values to discuss multicollinearity.
```{r}

#Calcualte variance inflation factors
vif(model.saturated)

#No one variable >5, so multicollinearity may not be a concern

```

7. A-V Plots: Create added variable plots for this model. What conclusions might you draw from these plots?
```{r, warning = FALSE}

avPlots(model.saturated)

#Added Variable plot
#Q: What is the effect of a predictor variable on our dependent variable while holding all other predictor variables constant? 
#Slopes indicate which variables have the strongest influence on price
#Results suggest Food and Decor matter the most

```

8. Simple Linear Regression: Fit a new simple linear regression that predicts the price of dinner from the service rating alone. Discuss this regression in light of your answer to part 6.
```{r}

model.simple = lm(Price ~ Service, data=restaurants)
summary(model.simple)

#Results: 
#model pval = 2.2e-16 (same as saturated model)
#R^2adj = 0.4075 (down from 0.6187)

```

## Question #2: NYC Restaurants Data
**Purpose: Understanding Model Selection and Comparison**

1. Fit Models: Try running two different models to compare with your saturated model. First, regress the price of dinner on food and decor. Then, regress the price of dinner on food, decor, and location. 
For each model, hit each of the questions described in Section 1.3.a-f above. Also check your MLR assumption diagnostics and comment on multicollinearity. 
```{r}

#Predictors: Food & Decor
model1 = lm(Price ~ Food+Decor, data = restaurants)
summary(model1)
  #Results:
  #Overall pval = 2.2e-16
  #Food, Decor, and Intercept > 0.05 
  #R^2adj = 0.6121


#Predictors: Food & Decor & Location
model2 = lm(Price ~ Food+Decor+Location, data = restaurants)
summary(model2)
  #Results:
  #Overall pval = 2.2e-16
  #Food, Decor, Location, and Intercept > 0.05 
  #R^2adj = 0.6211

```

2. Compare: Using the metrics described in Section 1 above (e.g. R-squared, RSE, coefficient magnitudes/significance, AIC/BIC, etc.) compare your different models. Which do you think is best?
```{r}

#Pvalue: Both models significant
#Predictors: All predictors significantly contribute to prediction
#R^2adj: Addition of Location minimally improved value
#ANOVA: Model2 significantly different than Model1
  anova(model1, model2) 

#AIC: Model2 < Model1 -> Model2 more accurate
  AIC(model1, model2)

#BIC: Model2 > Model1 -> Model1 just *slightly* lower than Model2
  BIC(model1, model2)



```

3. Conclude: Finally, draw some conclusions about the effect certain variables have on meal price.
```{r}

#price = -22.0 + 1.5*(food) + 1.9*(decor) + -2.1*(service)


#Increase of decor by 1 unit increases price by 1.9$
#Increase of food by 1 unit increases price by 1.5$


```

## Question #3: Machine Learning Theory
**Purpose: Demonstrate Theory of Lecture Material**

1. MLR Assumptions: What are the 5 assumptions of multiple linear regression?
```{r}

#Assumptions of MLR
#1. Linear relationship between response and predictor variables
#2. Homoscedascitity = Constant variance
#3. Normal distribution of residual variance
#4. Predictors are not multicollinear
#5. Residuals are independent from each other






```

2. Multicollinearity: When multicollinearity exists, what can happen to your regression coefficients and standard errors?
```{r}

#Multicollinearity introduces redundancies within our predictors. 

#This makes it more difficult to make inferences about relationships between predictors and response.

#Inflates standard errors

#Reduces power and reliability of the regresssion model


```


## Question #4: Challenge Question
1. We have discussed the false positive issue with hypothesis test. To understand this phenomenon, randomly generate a vector x and y = 0*x +residual. Regress y on x and then perform t-test to see if x is significant. Repeat the process multiple times, what is the probability that the t-test indicates significance?
```{r}


```